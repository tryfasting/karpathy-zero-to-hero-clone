{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 1,\n",
    "   \"id\": \"5e528810\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import torch\\n\",\n",
    "    \"import torch.nn as nn\\n\",\n",
    "    \"import torch.nn.functional as F\\n\",\n",
    "    \"import matplotlib.pyplot as plt # for making figures\\n\",\n",
    "    \"%matplotlib inline\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 2,\n",
    "   \"id\": \"dd540356\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"Using device : cuda\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"# GPU 설정\\n\",\n",
    "    \"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\\n\",\n",
    "    \"print(f'Using device : {device}')\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 3,\n",
    "   \"id\": \"c1349fa5\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# cuDNN 최적화 \\n\",\n",
    "    \"torch.backends.cudnn.benchmark = True\\n\",\n",
    "    \"torch.backends.cudnn.enabled = True\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 4,\n",
    "   \"id\": \"4780c521\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# 모든 변수 CUDA 상태 일괄 확인\\n\",\n",
    "    \"\\n\",\n",
    "    \"def check_all_cuda_status():\\n\",\n",
    "    \"    '''현재 namespace의 모든 변수 CUDA 상태 확인'''\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # 전역 변수 가져오기\\n\",\n",
    "    \"    global_vars = globals()\\n\",\n",
    "    \"\\n\",\n",
    "    \"    cuda_vars = []\\n\",\n",
    "    \"    cpu_vars = []\\n\",\n",
    "    \"    non_tensor_vars = []\\n\",\n",
    "    \"\\n\",\n",
    "    \"    for var_name, var_value in global_vars.items():\\n\",\n",
    "    \"        # 시스템 변수 제외\\n\",\n",
    "    \"        if var_name.startswith('_'):\\n\",\n",
    "    \"            continue\\n\",\n",
    "    \"            \\n\",\n",
    "    \"        # PyTorch 텐서인지 확인\\n\",\n",
    "    \"        if isinstance(var_value, torch.Tensor):\\n\",\n",
    "    \"            if var_value.is_cuda:\\n\",\n",
    "    \"                cuda_vars.append(var_name)\\n\",\n",
    "    \"            else:\\n\",\n",
    "    \"                cpu_vars.append(var_name)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # PyTorch 모델인지 확인\\n\",\n",
    "    \"        elif isinstance(var_value, nn.Module):\\n\",\n",
    "    \"            try:\\n\",\n",
    "    \"                if next(var_value.parameters()).is_cuda:\\n\",\n",
    "    \"                    cuda_vars.append(f\\\"{var_name} (model)\\\")\\n\",\n",
    "    \"                else:\\n\",\n",
    "    \"                    cpu_vars.append(f\\\"{var_name} (model)\\\")\\n\",\n",
    "    \"            except StopIteration:\\n\",\n",
    "    \"                # 파라미터가 없는 모델\\n\",\n",
    "    \"                non_tensor_vars.append(f\\\"{var_name} (empty model)\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # 기타 변수들\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            non_tensor_vars.append(var_name)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # 결과 출력\\n\",\n",
    "    \"    print(\\\"=== CUDA 상태 확인 결과 ===\\\")\\n\",\n",
    "    \"    print(f\\\"\\\\n✅ CUDA에 있는 변수들 ({len(cuda_vars)}개):\\\")\\n\",\n",
    "    \"    for var in cuda_vars:\\n\",\n",
    "    \"        print(f\\\"  - {var}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"\\\\n❌ CPU에 있는 변수들 ({len(cpu_vars)}개):\\\")\\n\",\n",
    "    \"    for var in cpu_vars:\\n\",\n",
    "    \"        print(f\\\"  - {var}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"\\\\n➖ 텐서/모델이 아닌 변수들 ({len(non_tensor_vars)}개):\\\")\\n\",\n",
    "    \"    for var in non_tensor_vars[:10]:  # 처음 10개만 표시\\n\",\n",
    "    \"        print(f\\\"  - {var}\\\")\\n\",\n",
    "    \"    if len(non_tensor_vars) > 10:\\n\",\n",
    "    \"        print(f\\\"  ... 및 {len(non_tensor_vars) - 10}개 더\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return cuda_vars, cpu_vars, non_tensor_vars\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 5,\n",
    "   \"id\": \"a08e7b38\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# check_all_cuda_status()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 6,\n",
    "   \"id\": \"78cec006\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/plain\": [\n",
    "       \"30\"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 6,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"import gc\\n\",\n",
    "    \"# cuda cache 정리\\n\",\n",
    "    \"torch.cuda.empty_cache()\\n\",\n",
    "    \"gc.collect()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 7,\n",
    "   \"id\": \"9126965d\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"32033\\n\",\n",
    "      \"15\\n\",\n",
    "      \"['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"# read in all the words\\n\",\n",
    "    \"words = open('../data/names.txt', 'r').read().splitlines()\\n\",\n",
    "    \"print(len(words))\\n\",\n",
    "    \"print(max(len(w) for w in words))\\n\",\n",
    "    \"print(words[:8])\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 8,\n",
    "   \"id\": \"8e1a8fb7\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\\n\",\n",
    "      \"27\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"# build the vocabulary of characters and mapping to/from integers\\n\",\n",
    "    \"chars = sorted(list(set(''.join(words))))\\n\",\n",
    "    \"stoi = {s:i+1 for i,s in enumerate(chars)}\\n\",\n",
    "    \"stoi['.'] = 0\\n\",\n",
    "    \"itos = {i:s for s,i in stoi.items()}\\n\",\n",
    "    \"vocab_size = len(itos)\\n\",\n",
    "    \"print(itos)\\n\",\n",
    "    \"print(vocab_size)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 9,\n",
    "   \"id\": \"0a30b58c\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# shuffle up the words\\n\",\n",
    "    \"import random\\n\",\n",
    "    \"random.seed(42)\\n\",\n",
    "    \"random.shuffle(words)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 38,\n",
    "   \"id\": \"22e4d92b\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"torch.Size([182437, 8]) torch.Size([182437])\\n\",\n",
    "      \"torch.Size([22781, 8]) torch.Size([22781])\\n\",\n",
    "      \"torch.Size([22928, 8]) torch.Size([22928])\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"# build the dataset\\n\",\n",
    "    \"block_size = 8 # context length : how many characters do we take to predict the next one?\\n\",\n",
    "    \"def build_dataset(words):\\n\",\n",
    "    \"    X,Y = [], []\\n\",\n",
    "    \"    for w in words:\\n\",\n",
    "    \"        # print(w)\\n\",\n",
    "    \"        context = [0] * block_size\\n\",\n",
    "    \"        for ch in w + '.':\\n\",\n",
    "    \"            ix = stoi[ch]\\n\",\n",
    "    \"            X.append(context)  \\n\",\n",
    "    \"            Y.append(ix)\\n\",\n",
    "    \"            context = context[1:] + [ix] # crop and append\\n\",\n",
    "    \"    X = torch.tensor(X) \\n\",\n",
    "    \"    Y = torch.tensor(Y)\\n\",\n",
    "    \"    print(X.shape, Y.shape)\\n\",\n",
    "    \"    return X,Y\\n\",\n",
    "    \"\\n\",\n",
    "    \"import random \\n\",\n",
    "    \"random.seed(42)\\n\",\n",
    "    \"random.shuffle(words)\\n\",\n",
    "    \"n1 = int(0.8*len(words))\\n\",\n",
    "    \"n2 = int(0.9*len(words))\\n\",\n",
    "    \"\\n\",\n",
    "    \"Xtr, Ytr = build_dataset(words[:n1])      # 80% training set\\n\",\n",
    "    \"Xdev, Ydev = build_dataset(words[n1:n2])  # 10% dev=validation set\\n\",\n",
    "    \"Xte, Yte = build_dataset(words[n2:])      # 10% test set\\n\",\n",
    "    \"\\n\",\n",
    "    \"Xtr = Xtr.to(device)\\n\",\n",
    "    \"Ytr = Ytr.to(device)\\n\",\n",
    "    \"Xdev = Xdev.to(device)\\n\",\n",
    "    \"Ydev = Ydev.to(device)\\n\",\n",
    "    \"Xte = Xte.to(device)\\n\",\n",
    "    \"Yte = Yte.to(device)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 39,\n",
    "   \"id\": \"34890338\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"........ --> t\\n\",\n",
    "      \".......t --> a\\n\",\n",
    "      \"......ta --> u\\n\",\n",
    "      \".....tau --> r\\n\",\n",
    "      \"....taur --> e\\n\",\n",
    "      \"...taure --> n\\n\",\n",
    "      \"..tauren --> .\\n\",\n",
    "      \"........ --> s\\n\",\n",
    "      \".......s --> u\\n\",\n",
    "      \"......su --> l\\n\",\n",
    "      \".....sul --> e\\n\",\n",
    "      \"....sule --> m\\n\",\n",
    "      \"...sulem --> a\\n\",\n",
    "      \"..sulema --> n\\n\",\n",
    "      \".suleman --> .\\n\",\n",
    "      \"........ --> z\\n\",\n",
    "      \".......z --> e\\n\",\n",
    "      \"......ze --> r\\n\",\n",
    "      \".....zer --> e\\n\",\n",
    "      \"....zere --> n\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"for x,y in zip(Xtr[:20],Ytr[:20]):\\n\",\n",
    "    \"    print(''.join(itos[ix.item()] for ix in x), '-->', itos[y.item()])\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 40,\n",
    "   \"id\": \"b10ef3d8\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Near copy paste of the layers we have developed in Part 3\\n\",\n",
    "    \"\\n\",\n",
    "    \"# -----------------------------------------------------\\n\",\n",
    "    \"class Linear:\\n\",\n",
    "    \"    def __init__(self, fan_in, fan_out, bias = True):\\n\",\n",
    "    \"        self.weight = torch.randn((fan_in, fan_out)) / fan_in**0.5 # note : kaiming init\\n\",\n",
    "    \"        self.bias = torch.zeros(fan_out) if bias else None\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def __call__(self, x):\\n\",\n",
    "    \"        self.out = x @ self.weight\\n\",\n",
    "    \"        if self.bias is not None:\\n\",\n",
    "    \"            self.out += self.bias\\n\",\n",
    "    \"        return self.out\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def parameters(self):\\n\",\n",
    "    \"        return [self.weight] + ([] if self.bias is None else [self.bias])\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def to(self, device):\\n\",\n",
    "    \"        self.weight = self.weight.to(device)\\n\",\n",
    "    \"        if self.bias is not None:\\n\",\n",
    "    \"            self.bias = self.bias.to(device)\\n\",\n",
    "    \"        return self\\n\",\n",
    "    \"    \\n\",\n",
    "    \"# --------------------------------------------------------------------\\n\",\n",
    "    \"class BatchNorm1d:\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def __init__(self, dim, eps=1e-5, momentum=0.1):\\n\",\n",
    "    \"        self.eps = eps\\n\",\n",
    "    \"        self.momentum = momentum\\n\",\n",
    "    \"        self.training = True\\n\",\n",
    "    \"        # parameters (trained with backprop)\\n\",\n",
    "    \"        self.gamma = torch.ones(dim)\\n\",\n",
    "    \"        self.beta = torch.zeros(dim)\\n\",\n",
    "    \"        # buffers (trained with a running 'momentum update')\\n\",\n",
    "    \"        self.running_mean = torch.zeros(dim)\\n\",\n",
    "    \"        self.running_var = torch.ones(dim)\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def __call__(self, x):\\n\",\n",
    "    \"        # calculate the forward pass\\n\",\n",
    "    \"        if self.training:\\n\",\n",
    "    \"            if x.ndim == 2:\\n\",\n",
    "    \"                dim = 0\\n\",\n",
    "    \"            elif x.ndim == 3:\\n\",\n",
    "    \"                dim = (0,1)\\n\",\n",
    "    \"            xmean = x.mean(0, keepdim=True) # batch mean\\n\",\n",
    "    \"            xvar = x.var(0, keepdim=True, unbiased=True) # batch variance\\n\",\n",
    "    \"        else:\\n\",\n",
    "    \"            xmean = self.running_mean\\n\",\n",
    "    \"            xvar = self.running_var\\n\",\n",
    "    \"        xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\\n\",\n",
    "    \"        self.out = self.gamma * xhat + self.beta\\n\",\n",
    "    \"        # update the buffers\\n\",\n",
    "    \"        if self.training:\\n\",\n",
    "    \"            with torch.no_grad():\\n\",\n",
    "    \"                self.running_mean = (1-self.momentum) * self.running_mean + self.momentum * xmean\\n\",\n",
    "    \"                self.running_var = (1-self.momentum) * self.running_var + self.momentum * xvar\\n\",\n",
    "    \"        return self.out\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def parameters(self):\\n\",\n",
    "    \"        return [self.gamma, self.beta]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def to(self, device):\\n\",\n",
    "    \"        self.gamma = self.gamma.to(device)\\n\",\n",
    "    \"        self.beta = self.beta.to(device)\\n\",\n",
    "    \"        self.running_mean = self.running_mean.to(device)\\n\",\n",
    "    \"        self.running_var = self.running_var.to(device)\\n\",\n",
    "    \"        return self\\n\",\n",
    "    \"    \\n\",\n",
    "    \"#------------------------------------------------------------------- \\n\",\n",
    "    \"class Tanh:\\n\",\n",
    "    \"    def __call__(self,x):\\n\",\n",
    "    \"        self.out = torch.tanh(x)\\n\",\n",
    "    \"        return self.out\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def parameters(self):\\n\",\n",
    "    \"        return []   \\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def to(self, device):\\n\",\n",
    "    \"        return self \\n\",\n",
    "    \"\\n\",\n",
    "    \"#-------------------------------------------------------------------- \\n\",\n",
    "    \"class Embedding:\\n\",\n",
    "    \"    def __init__(self, num_embeddings, embedding_dim):\\n\",\n",
    "    \"        self.weight = torch.randn((num_embeddings, embedding_dim))\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def __call__(self,IX):\\n\",\n",
    "    \"        self.out = self.weight[IX]\\n\",\n",
    "    \"        return self.out\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def parameters(self):\\n\",\n",
    "    \"        return [self.weight]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def to(self, device):\\n\",\n",
    "    \"        self.weight = self.weight.to(device)\\n\",\n",
    "    \"        return self\\n\",\n",
    "    \"    \\n\",\n",
    "    \"# --------------------------------------------------------------------\\n\",\n",
    "    \"class Flatten:\\n\",\n",
    "    \"    def __call__(self,x):\\n\",\n",
    "    \"        self.out = x.view(x.shape[0], -1)\\n\",\n",
    "    \"        return self.out\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def parameters(self):\\n\",\n",
    "    \"        return []  \\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def to(self, device):\\n\",\n",
    "    \"        return self \\n\",\n",
    "    \"    \\n\",\n",
    "    \"# --------------------------------------------------------------------\\n\",\n",
    "    \"class Sequential:\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def __init__(self, layers):\\n\",\n",
    "    \"        '''받은 layer list를 객체의 속성으로 저장한다.'''\\n\",\n",
    "    \"        self.layers = layers\\n\",\n",
    "    \"\\n\",\n",
    "    \"    def __call__(self, x):\\n\",\n",
    "    \"        '''객체를 함수처럼 호출하게 해준다.'''\\n\",\n",
    "    \"        for layer in self.layers:\\n\",\n",
    "    \"            # 첫 번째 레이어의 출력이 두 번째 레이어의 입력이 되는 식으로, x를 update하면서 연쇄적으로 처리한다.\\n\",\n",
    "    \"            x = layer(x)\\n\",\n",
    "    \"        self.out = x\\n\",\n",
    "    \"        return self.out\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def parameters(self):\\n\",\n",
    "    \"        # get parameters of all layers and stretch them out into one list\\n\",\n",
    "    \"        return [p for layer in self.layers for p in layer.parameters()]\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    def to(self, device):\\n\",\n",
    "    \"        for layer in self.layers:\\n\",\n",
    "    \"            layer.to(device)\\n\",\n",
    "    \"        return self\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 41,\n",
    "   \"id\": \"3181d3be\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"torch.manual_seed(42);\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 42,\n",
    "   \"id\": \"07d90f7f\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"22097\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"n_embd = 10 # the dimensionality of the character embedding vectors\\n\",\n",
    "    \"n_hidden = 200 # the number of neurons in the hidden layer of the MLP\\n\",\n",
    "    \"\\n\",\n",
    "    \"model = Sequential([\\n\",\n",
    "    \"    Embedding(vocab_size, n_embd),\\n\",\n",
    "    \"    Flatten(),\\n\",\n",
    "    \"    Linear(n_embd * block_size, n_hidden, bias=False), BatchNorm1d(n_hidden), Tanh(),\\n\",\n",
    "    \"    Linear(n_hidden, vocab_size),\\n\",\n",
    "    \"])\\n\",\n",
    "    \"\\n\",\n",
    "    \"# 모든 layer를 device로 이동\\n\",\n",
    "    \"model.to(device)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# parameter init\\n\",\n",
    "    \"with torch.no_grad():\\n\",\n",
    "    \"    model.layers[-1].weight *= 0.1 # last layer make less confident\\n\",\n",
    "    \"\\n\",\n",
    "    \"parameters = model.parameters()\\n\",\n",
    "    \"print(sum(p.nelement() for p in parameters)) # number of parameters in total\\n\",\n",
    "    \"\\n\",\n",
    "    \"for p in parameters:\\n\",\n",
    "    \"    p.requires_grad = True\\n\",\n",
    "    \"\\n\",\n",
    "    \"for i, p in enumerate(parameters):\\n\",\n",
    "    \"    p = p.to(device)  \"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"71b7f4dc\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"torch.Size([4, 8])\\n\"\n",
    "     ]\n",
    "    },\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/plain\": [\n",
    "       \"tensor([[ 0,  0,  0,  0,  0,  0,  0, 14],\\n\",\n",
    "       \"        [ 0,  0,  0, 19,  5, 14,  1, 25],\\n\",\n",
    "       \"        [ 0, 15, 12, 25, 13, 16,  9,  1],\\n\",\n",
    "       \"        [ 0,  0,  0, 20, 18,  9, 20, 15]], device='cuda:0')\"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 44,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"ix = torch.randint(0, Xtr.shape[0], (4,)) # let's look at a batch of just 4 examples\\n\",\n",
    "    \"Xb, Yb = Xtr[ix], Ytr[ix]\\n\",\n",
    "    \"logits = model(Xb)\\n\",\n",
    "    \"print(Xb.shape)\\n\",\n",
    "    \"# [4,8] : 4 because we picked four numbers, 8 is block_size\\n\",\n",
    "    \"Xb\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"90d5bb10\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/plain\": [\n",
    "       \"torch.Size([4, 8, 10])\"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 45,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"model.layers[0].out.shape # output of Embedding layer\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"9cbdba73\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/plain\": [\n",
    "       \"torch.Size([4, 80])\"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 46,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"model.layers[1].out.shape # output of Flatten layer\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"03441b1a\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/plain\": [\n",
    "       \"torch.Size([4, 200])\"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 47,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"model.layers[2].out.shape # output of Linear layer\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"id\": \"b71c3c2c\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/plain\": [\n",
    "       \"torch.Size([4, 200])\"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 49,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"(torch.randn(4,4,20) @ torch.randn(20,200) + torch.randn(200)).shape\\n\",\n",
    "    \"# How it works?\\n\",\n",
    "    \"# In the Pytorch, the matrix multiplication only works on the last Dimension.\\n\",\n",
    "    \"# (for upward example torch.randn(4,5,2,80), it is 80)\\n\",\n",
    "    \"# and the dimension before it in the input tensor are left unchanged.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 43,\n",
    "   \"id\": \"286acc4a\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"      0/ 200000: 3.3041\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"# same optimization as last time\\n\",\n",
    "    \"max_steps = 200000\\n\",\n",
    "    \"batch_size = 32\\n\",\n",
    "    \"lossi = []\\n\",\n",
    "    \"\\n\",\n",
    "    \"for i in range(max_steps):\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # minibatch construct\\n\",\n",
    "    \"    ix = torch.randint(0, Xtr.shape[0], (batch_size,))\\n\",\n",
    "    \"    ix = ix.to(device)\\n\",\n",
    "    \"    Xb,Yb = Xtr[ix], Ytr[ix] # batch X,Y\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # forward pass\\n\",\n",
    "    \"    logits = model(Xb)\\n\",\n",
    "    \"    loss = F.cross_entropy(logits, Yb) # loss function\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # backward pass\\n\",\n",
    "    \"    for p in parameters:\\n\",\n",
    "    \"        p.grad = None\\n\",\n",
    "    \"    loss.backward()\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # update: simple SGD\\n\",\n",
    "    \"    lr = 0.1 if i < 150000 else 0.01 # step learning rate decay\\n\",\n",
    "    \"    for p in parameters:\\n\",\n",
    "    \"        p.data += -lr * p.grad\\n\",\n",
    "    \"\\n\",\n",
    "    \"    # track stats\\n\",\n",
    "    \"    if i % 10000 == 0: # print every once in a while\\n\",\n",
    "    \"        print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\\n\",\n",
    "    \"    lossi.append(loss.log10().item())\\n\",\n",
    "    \"\\n\",\n",
    "    \"    break\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 24,\n",
    "   \"id\": \"0d528ef1\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"ename\": \"RuntimeError\",\n",
    "     \"evalue\": \"shape '[-1, 1000]' is invalid for input of size 1\",\n",
    "     \"output_type\": \"error\",\n",
    "     \"traceback\": [\n",
    "      \"\\u001b[31m---------------------------------------------------------------------------\\u001b[39m\",\n",
    "      \"\\u001b[31mRuntimeError\\u001b[39m                              Traceback (most recent call last)\",\n",
    "      \"\\u001b[36mCell\\u001b[39m\\u001b[36m \\u001b[39m\\u001b[32mIn[24]\\u001b[39m\\u001b[32m, line 4\\u001b[39m\\n\\u001b[32m      1\\u001b[39m \\u001b[38;5;66;03m# torch.Size([200, 1000]) 열방향(행)으로 합산해서,총 200행, 평균을 구함.\\u001b[39;00m\\n\\u001b[32m      2\\u001b[39m \\u001b[38;5;66;03m# 매 1000번의 loss.log10()를 평균 낸것과 같음\\u001b[39;00m\\n\\u001b[32m      3\\u001b[39m \\u001b[38;5;66;03m# 150, 즉 150000번째에 learning decay\\u001b[39;00m\\n\\u001b[32m----> \\u001b[39m\\u001b[32m4\\u001b[39m plt.plot(\\u001b[43mtorch\\u001b[49m\\u001b[43m.\\u001b[49m\\u001b[43mtensor\\u001b[49m\\u001b[43m(\\u001b[49m\\u001b[43mlossi\\u001b[49m\\u001b[43m)\\u001b[49m\\u001b[43m.\\u001b[49m\\u001b[43mview\\u001b[49m\\u001b[43m(\\u001b[49m\\u001b[43m-\\u001b[49m\\u001b[32;43m1\\u001b[39;49m\\u001b[43m,\\u001b[49m\\u001b[32;43m1000\\u001b[39;49m\\u001b[43m)\\u001b[49m.mean(\\u001b[32m1\\u001b[39m))\\n\",\n",
    "      \"\\u001b[31mRuntimeError\\u001b[39m: shape '[-1, 1000]' is invalid for input of size 1\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"# torch.Size([200, 1000]) 열방향(행)으로 합산해서,총 200행, 평균을 구함.\\n\",\n",
    "    \"# 매 1000번의 loss.log10()를 평균 낸것과 같음\\n\",\n",
    "    \"# 150, 즉 150000번째에 learning decay\\n\",\n",
    "    \"plt.plot(torch.tensor(lossi).view(-1,1000).mean(1))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 35,\n",
    "   \"id\": \"ac0487fb\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# put layers into eval mode (needed for batchroom expecially)\\n\",\n",
    "    \"for layer in model.layers:\\n\",\n",
    "    \"    layer.training = False\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 36,\n",
    "   \"id\": \"d8d3f0f4\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"train 3.292391777038574\\n\",\n",
    "      \"val 3.291624069213867\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"# evaluate the loss\\n\",\n",
    "    \"@torch.no_grad() # this decorator disables gradient tracking\\n\",\n",
    "    \"def split_loss(split):\\n\",\n",
    "    \"  x,y = {\\n\",\n",
    "    \"    'train': (Xtr, Ytr),\\n\",\n",
    "    \"    'val': (Xdev, Ydev),\\n\",\n",
    "    \"    'test': (Xte, Yte),\\n\",\n",
    "    \"  }[split]\\n\",\n",
    "    \"  logits = model(x)\\n\",\n",
    "    \"  loss = F.cross_entropy(logits, y)\\n\",\n",
    "    \"  print(split, loss.item())\\n\",\n",
    "    \"\\n\",\n",
    "    \"split_loss('train')\\n\",\n",
    "    \"split_loss('val')\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"id\": \"8832277d\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"### Performance log\\n\",\n",
    "    \"- original (3 character context + 200 hidden neurons, 12k params): train 2.058, val 2.105\\n\",\n",
    "    \"- context: 3 -> 8 (22k params): train 1.918, val 2.027\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 37,\n",
    "   \"id\": \"552ea7ca\",\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"vritajokyvgrzppeopkgbr.\\n\",\n",
    "      \"eqwskmairiphqiojfibjxqivf.\\n\",\n",
    "      \"btuupwhoqxgzmiirwxifbqoionyuqucicpmoph.\\n\",\n",
    "      \"q.\\n\",\n",
    "      \"smqolvhqsugnqmkfzwltv.\\n\",\n",
    "      \"vlhcvqdstwdnzqorkeezuruedg.\\n\",\n",
    "      \"hzdikhtptrynfpzykyrqjytrjyjjnfwkzgtaoyrzzrzrrojdgceenn.\\n\",\n",
    "      \"qqzligtgjpfvddzarifpkzpkvgdcxslcncofrnclekelcfkgpplrgywuzjqckspcnpsjwzgkbcvxkdgacqotnlzxcrqzdkgvckzkl.\\n\",\n",
    "      \".\\n\",\n",
    "      \"jcxa.\\n\",\n",
    "      \"socssekuhdvqnf.\\n\",\n",
    "      \"ejpmacbgm.\\n\",\n",
    "      \"tiewwplkfxsgkmplduljhqyxztuepoyabrcvelvvkjzabzqmgkhnv.\\n\",\n",
    "      \"vynpsetlgsldne.\\n\",\n",
    "      \"hdhez.\\n\",\n",
    "      \"paixoikvqjnasoxsufjtwnxcpwybftyzylaeicmrhrphmllgpugsxuguuflgcbbuusnzor.\\n\",\n",
    "      \"hsneawwrwuisfwgrmnqgftlowzhgmgllhfmdmjdijzvhqclkeabbxikunojephk.\\n\",\n",
    "      \"epftdrmibrzpxubijf.\\n\",\n",
    "      \"fsxkrcgqxskimptqmjxmvoekhcb.\\n\",\n",
    "      \"wgrccdftwclykglkthdtluexyvzjyudkfzyknvmkbxkakvlffvccehtwzlivfkdkddvn.\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"# sample from the model\\n\",\n",
    "    \"g = torch.Generator().manual_seed(2147483647 + 10)\\n\",\n",
    "    \"\\n\",\n",
    "    \"for _ in range(20):\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    out = []\\n\",\n",
    "    \"    context = [0] * block_size # initialize with all ...\\n\",\n",
    "    \"    while True:\\n\",\n",
    "    \"      # forward pass the neural net\\n\",\n",
    "    \"      logits = model(torch.tensor([context]))\\n\",\n",
    "    \"      probs = F.softmax(logits, dim=1)\\n\",\n",
    "    \"      # sample from the distribution\\n\",\n",
    "    \"      ix = torch.multinomial(probs, num_samples=1).item()\\n\",\n",
    "    \"      # shift the context window and track the samples\\n\",\n",
    "    \"      context = context[1:] + [ix]\\n\",\n",
    "    \"      out.append(ix)\\n\",\n",
    "    \"      # if we sample the special '.' token, break\\n\",\n",
    "    \"      if ix == 0:\\n\",\n",
    "    \"        break\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(''.join(itos[i] for i in out)) # decode and print the generated word\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \".venv\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.12.10\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 5\n",
    "}\n"
   ],
   "id": "38bde047cd33d984"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
